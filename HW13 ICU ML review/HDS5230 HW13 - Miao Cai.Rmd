---
title: |
  | HSD5230 High Performance Computing
  | Homework 13
author: Miao Cai^[Department of Epidemiology and Biostatistics, College for Public
  Health and Social Justice, Saint Louis University. Email [miao.cai@slu.edu](mailto:miao.cai@slu.edu)]
date: "`r Sys.Date()`"
output:
  pdf_document: default
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{enumitem}
  - \usepackage{soul}
  - \newcommand{\benum}{\begin{enumerate}[label=(\alph*)]}
  - \newcommand{\ennum}{\end{enumerate}}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Read both the main paper and the appendix then answer the questions below.

> Nemati S, Holder A, Razmi F, Stanley MD, Clifford GD, Buchman TG. An Interpretable Machine Learning Model for Accurate Prediction of Sepsis in the ICU. Crit Care Med. 2018;46(4):547-553. doi:10.1097/CCM.0000000000002936

**
\fbox{\begin{minipage}{\linewidth}
1. Provide a brief summary (1 paragraph) of this paper.
\benum
\item What was the hypothesis or goal of the paper?
\item What was the outcome of the study?
\ennum
\end{minipage}}
**

The goal of this paper is to develop and validate an Artificial Intelligence Sepsis Expert algorithm using electronic medical records and high frequency physiologic data for early prediction of sepsis. The outcome variable was whether sepsis occurred to the patient or not.




\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
2. What statistic did the authors use to summarize the model performance (what classification metric)? Name 2 other potential statistics they could have chosen. What are the advantages/disadvantages of the one they chose compared with the other two you came up with?
\end{minipage}}
**

The authors used the area under receiver operating characteristic (AUROC), as well as specificity (1-false alarm rate) and accuracy at a fixed 85% sensitivity level to summarize model performance.

The other two potential statistics can be:

- mean square error
- loglikelihood

The ones that I choose are more general statistics, not statistics specific to survival analysis models. The statistics that the authors used were specific to the modified Weibull-Cox proportional hazard model in this study.



\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
3. Did the study authors use a validation or testing dataset? Describe their approach to validation and testing. Do you think that is a good approach? Why or why not?
\end{minipage}}
**

The authors used a testing dataset. They randomly split the data into 80% training and 20% testing datasets, and calculate AUROC statistisc for both training and testing sets. This may not be a good approach since one-time random split may be subject to sampling error. A better approach is cross-validation.



\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
4. Would you describe their modeling approach as high or low capacity? Why? Support your answer.
\end{minipage}}
**

I would say this is a high-capacity model since they have 65 features in this dataset, which is relatively high compared to a typical 10-variable model, although they have quite a few patients (n = 27,537).





\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
5. What optimization algorithm did the authors use for model convergence?
\end{minipage}}
**

They used a mini-batch stochastic gradient descent approach to maximize the log likelihood of the data.




\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
6. Did the authors regularize the model? What sort of regularization did they use?
\end{minipage}}
**

Yes, they used L1 - L2 regularization to minimize overfitting and optimize generalizability of the learned model.




\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
7. Review the appendix graph summarizing the testing and training error. Do you think this model overfits the training data? Why or why not?
\end{minipage}}
**

No. The training set and testing set have a close agreement, which indicates good generalizability and no overfitting issue.





\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
8. How did the authors calculate variable importance? Why is this important to do?
\end{minipage}}
**

They used AUROC, specificity, and accuracy to evaluate the relative importance of each variable. It is important since the authors want to know what variables can lead to better prediction.




\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
9. Why is this study/approach novel compared to other prior sepsis prediction models? The authors discuss this in the discussion.
\end{minipage}}
**

- Their algorithm is the first one to use data collected at different resolutions (low-resolution electronic medical records data and high-resolution BR and HR data).
- Their algorithm is the first one to demonstrate acceptable performance of sepsis prediction over incrementally longer time windows.




\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
10. The authors have a nice discussion of prediction versus interdiction in the context of defining model value. Summarize that discussion in your own words. Do you think this model is proven to be of high value based on this report? How do we know if a tool like this can affect patient outcomes?
\end{minipage}}
**

The algorithm is of high value since it can be used to identify the sources of sepsis, adapted and generalized to other hospitals, make real-time predictions for patients who have the rish of sepsis. These machine learning tools can be useful and affect patient outcomes if it has high prediction accuracy and specificity so that it can be used to guide clinical practice.