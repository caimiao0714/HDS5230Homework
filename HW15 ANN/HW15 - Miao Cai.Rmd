---
title: |
  | HSD5230 High Performance Computing
  | Homework 15
author: Miao Cai^[Department of Epidemiology and Biostatistics, College for Public
  Health and Social Justice, Saint Louis University. Email [miao.cai@slu.edu](mailto:miao.cai@slu.edu)]
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
link-citations: yes
linkcolor: blue
header-includes:
- \usepackage{enumitem}
- \usepackage{soul}
- \newcommand{\benum}{\begin{enumerate}[label=(\alph*)]}
- \newcommand{\ennum}{\end{enumerate}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**
\fbox{\begin{minipage}{\linewidth}
1. Depth versus width of a neural network
\benum
\item Define the depth and width of a neural network in lay terms.
\item How might you choose depth/width of a network?
\ennum
\end{minipage}}
**


- The number of layers (functions) is defined as the **depth** of a neural network. The complexity of the hidden layer (functions) is defined as the **width** of a neural network.
- We should start with a rough guess of the depth and width based on prior knowledge. Then try variations of the neural network and pick the best one based on the prediction performance (validation set error).








\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
2. Forward propagation
\benum
\item What is the final output from forward propagation?
\item What is the input?
\item Define forward propagation in lay terms.
\ennum
\end{minipage}}
**


- The final output from forward propagation is the cost function $J(\theta)$.
- The input is the data features $x$.
- The forward propagration is flowing from data features $x$, through neural network, and to the outcome $y$ until it reaches a scalar cost function $J(\theta)$.










\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
3. Back propagation
\benum
\item What is the goal of backpropagation? How does it differ from forward propagation?
\item Define backpropagation in lay terms.
\ennum
\end{minipage}}
**

- The goal of backpropagation is to calculat the gradient of cost function, which flows from the cost function $J(\theta)$ back to the inputs $x$. In comparison, forward propagation is flowing from inputs $x$ to cost function $J(\theta)$.
- Backpropagation is flowing from cost function $J(\theta)$, through neural networks, and back to inputs $x$ to calculate the gradients.








\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
4. Activation functions
\benum
\item Describe 3 different activation functions potentially used by a hidden layer. How do they differ?
\ennum
\end{minipage}}
**

- linear function: 
$$A(x) = xc$$
which gives a range of activations. The problem is the gradient is a constant, and since different linear layers can be replaced by a single year, we lost the ability of stacking layers.
- sigmoid function: 
$$A(x) = \frac{1}{1 + e^{-x}}$$
It has a non-linear form and smooth gradients, so it is a good activation function for a classifier. It also has the output ranged between 0 and 1, which will not be blown up. The problem with sigmoid function is that the gradient at the far side (both left and right) is very small, which causes the problem of "vanishing gradients".
- ReLu: 
$$A(x) = max(0, x)$$
It gives an output $x$ if $x$ is positive and 0 otherwise. It has sparse activation which is not so costly compared with the sigmoid function. The problem is that the gradient will be 0 when $x$ is nagative, and the state will stop responding to variations in error or input, which is also known as the "dying ReLu problem".

My solution is based on a great post by Avinash Sharma V at [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).





\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
5. Activation functions for output units versus hidden layers
\benum
\item Why is the choice of activation function for the output unit (output layer) more constrained compared to the activation functions for hidden layers? What considerations are there when choosing this activation function?
\ennum
\end{minipage}}
**

This is because the choice of activation function for the output depends on the type of the outputs. For example, if the output a classification, then the linear activation function is not a good choice since it is unbounded. When choosing the activation function for the output layer, we need to consider its data type.






\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
6. Universal approximation function theorem
\benum
\item Does the universal approximation function theorem mean we should prefer shallow, wide networks? Why or why not?
\ennum
\end{minipage}}
**

No. In the worst case, an exponential number of hidden layers may be required. A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. Therefore, we generally use deeper models that can reduce the number of units required to represent the desired function and can reduce the amount of generalization error.











\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
7. When implementing parameter norm penalties, we could fit a different penalty strength for every neuron or layer in the network. Why might we decide against that and fit the same penalty across all layers?
\end{minipage}}
**

It can be expensive to search for the correct value of multiple hyperparameters, so it is still reasonable to use the same weight decay at all layer just to reduce thte size of search space.












\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
8. When implementing a parameter norm penalty (like L1 or L2), do we penalize the bias terms for each layer, or just the weights? Why?
\end{minipage}}
**

 We do not induce too much variance by leaving the biases unregularized, while regularizing the bias parameters can induce a significant amount of underfitting. Therefore, we only penalize the weights of the affine transformation at each layer and leave the biases unregularized.









\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
9. What are the different effects of L1 versus L2 regularization in deep learning?
\end{minipage}}
**

Compared to $L^2$ regularization, $L^1$ regularization results in a solution that is more sparse. The sparsity here refers to the fact that some parameters have an optimal value of zero. This sparsity property of $L^1$ regularization has been extensively used as a feature selection mechanism.














\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
10. Explain why early stopping can be considered a form of regularization. How do we know when to stop?
\end{minipage}}
**

We can think of early stopping as a very efficient hyperparameter selection algorithm, with the number of training steps as another hyperparameter. We stop when no parameters have improved over the best recorded validation error for some pre-specified number of iterations.









\vspace{18pt}
**
\fbox{\begin{minipage}{\linewidth}
11. Pick one regularization technique covered in the chapter (other than parameter norm penalties like L1/L2 or early stopping). Briefly explain what the technique is and why it works to regularize the model.
\end{minipage}}
**

**Parameter sharing** is forcing sets of parameters to be equal. In this technique, we are interpreting various models ir model components as sharing a unique set of parameter. The major advantage is that only a subset of the parameters (the unique set) needs to be stored in memory.
















